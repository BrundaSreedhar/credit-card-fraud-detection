{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrundaSreedhar/credit-card-fraud-detection/blob/main/ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGFZZTRamvYd"
      },
      "source": [
        "# Initial EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAhgwcSMji5f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Install dependencies as needed:\n",
        "# pip install kagglehub[pandas-datasets]\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Set the path to the file you'd like to load\n",
        "file_path = \"creditcard.csv\"\n",
        "\n",
        "# Load the latest version\n",
        "df = kagglehub.dataset_load(\n",
        "    KaggleDatasetAdapter.PANDAS,\n",
        "  \"mlg-ulb/creditcardfraud\",\n",
        "  file_path,\n",
        "  # Provide any additional arguments like\n",
        "  # sql_query or pandas_kwargs. See the\n",
        "  # documenation for more information:\n",
        "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
        ")\n",
        "\n",
        "print(\"First 5 records:\")\n",
        "print(df.head())\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m00pAlUQ6mQE"
      },
      "outputs": [],
      "source": [
        "print('Missing values:', df.isnull().sum().sum())\n",
        "print('\\nDtypes:\\n', df.dtypes.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UiXWfp1iThC"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGdnMHQBq0Wp"
      },
      "source": [
        "It contains 284,807 credit card transactions with 31 features, including a target variable Class indicating whether a transaction is fraudulent or legitimate. Time represents the elapsed time between transactions and Amount indicates the transaction value. All features are numerical, and the dataset contains no missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRVLCD7Gj-Dy"
      },
      "outputs": [],
      "source": [
        "df['Class'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDdHMloJJWpF"
      },
      "outputs": [],
      "source": [
        "df['Class'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ckAbq4HkLYn"
      },
      "source": [
        "**Core Difficulty:** Dataset is imbalanced. Based on an initial analysis, approximately 99.81% of transactions are legitimate, while only 0.18% are fraudulent. This imbalance increases the risk of developing models that appear accurate but fail to identify fraudulent activity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6GUKZafliQA"
      },
      "outputs": [],
      "source": [
        "df[['Amount', 'Class']].groupby('Class').mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l8D2_Knmg5S"
      },
      "source": [
        "An initial comparison of transaction amounts shows that fraudulent transactions have a higher average transaction value (approximately 123) compared to legitimate transactions (approximately 90). While this suggests that transaction amount may be a useful feature for fraud detection, the overlap between classes indicates that accurate classification will require combining transaction amount with other anonymized features provided in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzhL7lakzcQL"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"Statistical Summary:\")\n",
        "print(\"-\"*80)\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3TeEhXpzwny"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count class distribution\n",
        "class_counts = df['Class'].value_counts()\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Bar chart\n",
        "axes[0].bar(['Legitimate (0)', 'Fraudulent (1)'], class_counts.values, color=['steelblue', 'crimson'], edgecolor='black')\n",
        "axes[0].set_ylabel('Number of Transactions')\n",
        "axes[0].set_title('Class Distribution (Count)')\n",
        "\n",
        "# Pie chart\n",
        "axes[1].pie(\n",
        "    class_counts.values,\n",
        "    labels=['Legitimate', 'Fraudulent'],\n",
        "    autopct='%1.3f%%',\n",
        "    startangle=90\n",
        ")\n",
        "axes[1].set_title('Class Distribution (Percentage)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print basic stats\n",
        "print(\"Legitimate:\", class_counts[0])\n",
        "print(\"Fraudulent:\", class_counts[1])\n",
        "print(\"Imbalance Ratio (Legit : Fraud) =\", round(class_counts[0] / class_counts[1], 0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6cs0_e1Jece"
      },
      "outputs": [],
      "source": [
        "df.hist(bins=30, figsize=(30, 30))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXPw66J_0dfB"
      },
      "source": [
        "Most of the other columns are roughly normally distributed around 0, which is expected as they're transformed already. Amounts are generally mostly small with some extreme values like 25000, these would be outliers. Time is a value between 0 and 172792 and its fairly distributed across the period, there are no heavy-tail outliers here. We preprocess the amount and time columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv8n5U4I7W5E"
      },
      "outputs": [],
      "source": [
        "# --- 2.4 PCA feature distributions: fraud vs legit ---\n",
        "v_features = [f'V{i}' for i in range(1, 29)]\n",
        "\n",
        "fig, axes = plt.subplots(7, 4, figsize=(20, 28))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feat in enumerate(v_features):\n",
        "    for label, color, name in [(0, 'steelblue', 'Legit'), (1, 'crimson', 'Fraud')]:\n",
        "        axes[i].hist(df[df['Class']==label][feat], bins=50, alpha=0.5,\n",
        "                     color=color, label=name, density=True)\n",
        "    axes[i].set_title(feat, fontsize=10)\n",
        "    axes[i].legend(fontsize=7)\n",
        "\n",
        "plt.suptitle('PCA Feature Distributions: Fraud vs Legit', fontsize=14, y=1.01)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6k6mp3_7me7"
      },
      "outputs": [],
      "source": [
        "# --- 2.5 Correlation heatmap (fraud transactions only) ---\n",
        "import seaborn as sns\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
        "\n",
        "for ax, label, title in [(axes[0], 0, 'Correlation — Legit'), (axes[1], 1, 'Correlation — Fraud')]:\n",
        "    corr = df[df['Class']==label][v_features + ['Amount']].corr()\n",
        "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "    sns.heatmap(corr, mask=mask, ax=ax, cmap='RdBu_r', center=0,\n",
        "                square=True, linewidths=0.5, annot=False, fmt='.1f')\n",
        "    ax.set_title(title, fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPDdoqGJC0R9"
      },
      "source": [
        "Naive, fast and model-free check of what could potentially be important features. This does not consider variance or correlations between features but might give us some intuition on which variables might influence why specific transactions were flagged as fraud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xon0xOGK7-Pr"
      },
      "outputs": [],
      "source": [
        "# --- 2.6 Feature importance preview: mean absolute difference between classes ---\n",
        "fraud = df[df['Class']==1][v_features].mean()\n",
        "legit = df[df['Class']==0][v_features].mean()\n",
        "diff = (fraud - legit).abs().sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "diff.plot(kind='bar', color='darkorange', edgecolor='black')\n",
        "plt.title('|Mean(Fraud) - Mean(Legit)| per Feature\\n(Higher = more separability)', fontsize=13)\n",
        "plt.ylabel('Absolute Mean Difference')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('Top 10 most separating features:')\n",
        "print(diff.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3wNzVwnDeqz"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "results = {}\n",
        "for feat in v_features:\n",
        "    stat, p = mannwhitneyu(\n",
        "        df[df['Class']==0][feat],\n",
        "        df[df['Class']==1][feat],\n",
        "        alternative='two-sided'\n",
        "    )\n",
        "    results[feat] = {'stat': stat, 'p_value': p}\n",
        "\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df['-log10(p)'] = -np.log10(results_df['p_value'])\n",
        "results_df = results_df.sort_values('-log10(p)', ascending=False)\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# --- Left: -log10(p-value) ---\n",
        "colors = ['crimson' if p < 0.05 else 'steelblue' for p in results_df['p_value']]\n",
        "axes[0].barh(results_df.index, results_df['-log10(p)'], color=colors, edgecolor='black')\n",
        "axes[0].axvline(-np.log10(0.05), color='black', linestyle='--', label='p=0.05')\n",
        "axes[0].set_xlabel('-log10(p-value)')\n",
        "axes[0].set_title('Mann-Whitney U: Feature Significance\\n(red = significant, higher = more significant)')\n",
        "axes[0].legend()\n",
        "\n",
        "# --- Right: U-statistic (normalized) ---\n",
        "n0 = (df['Class']==0).sum()\n",
        "n1 = (df['Class']==1).sum()\n",
        "results_df['U_norm'] = results_df['stat'] / (n0 * n1)  # ranges 0-1, 0.5 = no difference\n",
        "axes[1].barh(results_df.index, results_df['U_norm'], color='darkorange', edgecolor='black')\n",
        "axes[1].axvline(0.5, color='black', linestyle='--', label='No difference (0.5)')\n",
        "axes[1].set_xlabel('Normalized U-statistic')\n",
        "axes[1].set_title('Mann-Whitney U: Effect Size\\n(further from 0.5 = stronger separation)')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjVJHb0qEPYP"
      },
      "source": [
        "Left (-log10 p-value): which features are statistically different between fraud and legit. Almost all V features will be significant given the dataset size, so this alone isn't enough.\n",
        "\n",
        "\n",
        "Right (normalized U-statistic): the *effect size*, which is more meaningful here. It's essentially the probability that a random fraud transaction scores higher than a random legit one on that feature. Values near 0 or 1 mean strong separation; 0.5 means the feature is useless for discrimination.\n",
        "The right plot is helpful rank features by - significance without effect size is misleading with large datasets because even tiny, meaningless differences become statistically significant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW4V8B6d59ba"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6)) # Changed to 1 row, 2 columns, and adjusted figsize\n",
        "\n",
        "# Overall Amount Distribution (with log scale for better visibility)\n",
        "axes[0].hist(df['Amount'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "axes[0].set_xlabel('Transaction Amount ($)', fontsize=11, fontweight='bold')\n",
        "axes[0].set_ylabel('Frequency (log scale)', fontsize=11, fontweight='bold')\n",
        "axes[0].set_title('Distribution of Transaction Amounts (All)', fontsize=13, fontweight='bold')\n",
        "axes[0].set_yscale('log')\n",
        "axes[0].grid(alpha=0.3)\n",
        "df_legit=df[df['Class']==0]\n",
        "df_fraud=df[df['Class']==1]\n",
        "\n",
        "# Amount Distribution by Class\n",
        "# Box plot comparison\n",
        "box_data = [df_legit['Amount'], df_fraud['Amount']]\n",
        "bp = axes[1].boxplot(box_data, tick_labels=['Legitimate', 'Fraudulent'],\n",
        "                         patch_artist=True, showfliers=False)\n",
        "bp['boxes'][0].set_facecolor('#2ecc71')\n",
        "bp['boxes'][1].set_facecolor('#e74c3c')\n",
        "axes[1].set_ylabel('Transaction Amount ($)', fontsize=11, fontweight='bold')\n",
        "axes[1].set_title('Amount Distribution by Class (No Outliers)', fontsize=13, fontweight='bold')\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('02_amount_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Print statistics\n",
        "print(\"=\"*80)\n",
        "print(\"TRANSACTION AMOUNT STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Overall - Mean: ${df['Amount'].mean():.2f}, Median: ${df['Amount'].median():.2f}\")\n",
        "print(f\"Legitimate - Mean: ${df_legit['Amount'].mean():.2f}, Median: ${df_legit['Amount'].median():.2f}\")\n",
        "print(f\"Fraudulent - Mean: ${df_fraud['Amount'].mean():.2f}, Median: ${df_fraud['Amount'].median():.2f}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS-iPnXpJaKw"
      },
      "source": [
        "The transaction amount distribution is highly right-skewed, as shown by the histogram on a log scale, indicating that most transactions involve small amounts while a few very large transactions occur infrequently. When comparing transaction amounts by class using the box plot (with outliers removed), fraudulent transactions tend to have a lower median amount than legitimate ones, even though their mean is higher due to the presence of some high-value fraud cases. This suggests that fraud commonly occurs at smaller transaction amounts, possibly to avoid detection, while occasional large fraudulent transactions significantly increase the average."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYS8IfFp0GuG"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Transaction frequency over time\n",
        "axes[0, 0].hist(df['Time'], bins=100, color='teal', edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].set_xlabel('Time (seconds from first transaction)', fontsize=11, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Number of Transactions', fontsize=11, fontweight='bold')\n",
        "axes[0, 0].set_title('Transaction Frequency Over Time', fontsize=13, fontweight='bold')\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "# Time distribution by class\n",
        "axes[0, 1].hist(df_legit['Time'], bins=100, alpha=0.6, label='Legitimate',\n",
        "                color='#2ecc71', edgecolor='black')\n",
        "axes[0, 1].hist(df_fraud['Time'], bins=100, alpha=0.6, label='Fraudulent',\n",
        "                color='#e74c3c', edgecolor='black')\n",
        "axes[0, 1].set_xlabel('Time (seconds)', fontsize=11, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "axes[0, 1].set_title('Time Distribution by Class', fontsize=13, fontweight='bold')\n",
        "axes[0, 1].legend(fontsize=10)\n",
        "axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "# Fraud rate over time periods\n",
        "time_bins = pd.cut(df['Time'], bins=48)\n",
        "fraud_rate_time = df.groupby(time_bins)['Class'].agg(['mean', 'count'])\n",
        "fraud_rate_time['fraud_pct'] = fraud_rate_time['mean'] * 100\n",
        "\n",
        "axes[1, 0].plot(range(len(fraud_rate_time)), fraud_rate_time['fraud_pct'],\n",
        "                marker='o', linewidth=2, markersize=4, color='crimson')\n",
        "axes[1, 0].set_xlabel('Time Period', fontsize=11, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Fraud Rate (%)', fontsize=11, fontweight='bold')\n",
        "axes[1, 0].set_title('Fraud Rate Over Time Periods', fontsize=13, fontweight='bold')\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "# Scatter: Time vs Amount (sample for clarity)\n",
        "sample_size = min(10000, len(df))\n",
        "df_sample = df.sample(n=sample_size, random_state=42)\n",
        "scatter_legit = df_sample[df_sample['Class'] == 0]\n",
        "scatter_fraud = df_sample[df_sample['Class'] == 1]\n",
        "\n",
        "axes[1, 1].scatter(scatter_legit['Time'], scatter_legit['Amount'],\n",
        "                   alpha=0.3, s=10, c='#2ecc71', label='Legitimate')\n",
        "axes[1, 1].scatter(scatter_fraud['Time'], scatter_fraud['Amount'],\n",
        "                   alpha=0.8, s=30, c='#e74c3c', label='Fraudulent',\n",
        "                   edgecolors='black', linewidth=0.5)\n",
        "axes[1, 1].set_xlabel('Time (seconds)', fontsize=11, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Amount ($)', fontsize=11, fontweight='bold')\n",
        "axes[1, 1].set_title(f'Time vs Amount (Sample: {sample_size:,})', fontsize=13, fontweight='bold')\n",
        "axes[1, 1].legend(fontsize=10)\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('03_time_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\" Time distribution analysis complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPr3q8aaF7j3"
      },
      "source": [
        "The time-based visualizations reveal meaningful patterns in transaction behavior and fraud occurrence. Overall transaction frequency is not uniform across time, showing distinct peaks and low-activity periods, which suggests cyclic usage patterns. When comparing time distributions by class, fraudulent transactions broadly follow the same temporal structure as legitimate ones, indicating that fraud does not occur only at specific times but blends into normal activity. However, the fraud rate over time highlights certain periods with noticeable spikes, where the proportion of fraudulent transactions increases despite lower transaction counts. This suggests that fraud risk varies across time windows rather than volume alone. Additionally, the Time vs Amount scatter plot shows that fraudulent transactions tend to cluster at lower amounts but occasionally appear as higher-value outliers, reinforcing the need to consider time-based patterns alongside transaction amounts when detecting fraud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIAcWizSISzU"
      },
      "source": [
        "#Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlLOIeq2K9yv"
      },
      "outputs": [],
      "source": [
        "#Amount ranges from 0 to 25691.160000\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "new_df = df.copy()\n",
        "new_df['Amount'] = RobustScaler().fit_transform(new_df['Amount'].to_numpy().reshape(-1, 1))\n",
        "new_df['Amount'].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o60pOSlpLRVk"
      },
      "outputs": [],
      "source": [
        "new_df['Amount'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0T9MTwuLXGr"
      },
      "source": [
        "We now have a much smaller standard deviation, there are still outliers but its much better than what we had previously\n",
        "\n",
        "\n",
        "We'll just standardize Time since we dont seem to have any outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cxc_FC_LWib"
      },
      "outputs": [],
      "source": [
        "time = new_df['Time']\n",
        "#standard scaler\n",
        "new_df['Time'] = StandardScaler().fit_transform(new_df[['Time']])\n",
        "new_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcPKS4MTSqvb"
      },
      "outputs": [],
      "source": [
        "new_df = new_df.sample(frac=1, random_state=42)\n",
        "new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COGnJL7snaBf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, temp = train_test_split(\n",
        "    new_df,\n",
        "    test_size=0.2,\n",
        "    stratify=new_df['Class'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "test, val = train_test_split(\n",
        "    temp,\n",
        "    test_size=0.5,\n",
        "    stratify=temp['Class'],\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpBLmRt-TnCA"
      },
      "outputs": [],
      "source": [
        "x_train = train.drop(columns=['Class'])\n",
        "y_train = train['Class']\n",
        "\n",
        "x_test = test.drop(columns=['Class'])\n",
        "y_test = test['Class']\n",
        "\n",
        "x_val = val.drop(columns=['Class'])\n",
        "y_val = val['Class']\n",
        "\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape, x_val.shape, y_val.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO7geNhnT2H_"
      },
      "source": [
        "#Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqmdK-HQNFPs"
      },
      "source": [
        "Try Logistic Regression with and without class weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htau5oo8T4dX"
      },
      "outputs": [],
      "source": [
        "#logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "logistic_model = LogisticRegression()\n",
        "logistic_model.fit(x_train, y_train)\n",
        "logistic_model.score(x_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJg5X9cDUKOF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_val, logistic_model.predict(x_val), target_names=['Legit', 'Fraud']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uj3K4ClVNP3F"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "import numpy as np\n",
        "\n",
        "logistic_model_weighted = LogisticRegression(class_weight='balanced')\n",
        "logistic_model_weighted.fit(x_train, y_train)\n",
        "\n",
        "print(\"Before tuning threshold.. \")\n",
        "print(classification_report(y_val, logistic_model_weighted.predict(x_val), target_names=['Legit', 'Fraud']))\n",
        "\n",
        "probs = logistic_model_weighted.predict_proba(x_val)[:, 1]\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_val, probs)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Thresholds:\", thresholds)\n",
        "#choose threshold that maximizes F1\n",
        "f1 = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "best_idx = np.argmax(f1)\n",
        "best_threshold = thresholds[best_idx]\n",
        "\n",
        "print(\"Best threshold:\", best_threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRq81qhiPC5f"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "print(\"After tuning threshold.. \")\n",
        "y_pred = (probs >= best_threshold).astype(int)\n",
        "print(classification_report(y_val, y_pred))\n",
        "#print PR AUC\n",
        "print(\"PR AUC :\", average_precision_score(y_val, probs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkI0pZOeVU89"
      },
      "source": [
        "precisison here helps us understand the number of false positives. (Calling it a fraud when it wasnt a fraud)\n",
        "higher prec -> we dint flag real transactions\n",
        "\n",
        "\n",
        "Recall measures false negatives. it was fraud but we predicted not fraud. -> the model didnt predict that. Recall is important cause we want to catch the fraudulent transactions.\n",
        "\n",
        "Accuracy -> is it 100% accurate? no, we focus on the precision and recall. because of the imbalance in the dataset. accuracy is accurate if it was balanced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADC2f1QZgFAc"
      },
      "source": [
        "#Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqxGAdkgijuV"
      },
      "source": [
        "Experiments with and without SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5IJe5OAgH6C"
      },
      "outputs": [],
      "source": [
        "#Random forest to test the data\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_forest = RandomForestClassifier()\n",
        "random_forest.fit(x_train, y_train)\n",
        "\n",
        "print(classification_report(y_val, random_forest.predict(x_val), target_names=['Legit', 'Fraud']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zXzdPPjmCbS"
      },
      "outputs": [],
      "source": [
        "#try tuning threshold\n",
        "\n",
        "y_probs = random_forest.predict_proba(x_val)[:, 1]   # probability of fraud\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_val, y_probs)\n",
        "\n",
        "#choose threshold that maximizes F1\n",
        "f1 = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "best_idx = np.argmax(f1)\n",
        "best_threshold = thresholds[best_idx]\n",
        "\n",
        "print(\"Best threshold:\", best_threshold)\n",
        "\n",
        "print(\"After tuning threshold.. \")\n",
        "y_pred_tuned = (y_probs >= best_threshold).astype(int)\n",
        "print(classification_report(y_val, y_pred_tuned))\n",
        "print(\"PR AUC \", average_precision_score(y_val, y_probs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTpmU4DRn0aI"
      },
      "source": [
        "Calibrated Classfier CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGLf1zpZn6RI"
      },
      "outputs": [],
      "source": [
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "calibrated_rf = CalibratedClassifierCV(\n",
        "    random_forest,\n",
        "    method='sigmoid',\n",
        "    cv='prefit'\n",
        ")\n",
        "\n",
        "calibrated_rf.fit(x_val, y_val)\n",
        "\n",
        "y_probs = calibrated_rf.predict_proba(x_val)[:, 1]\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_val, y_probs)\n",
        "#choose threshold that maximizes F1\n",
        "f1 = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "best_idx = np.argmax(f1)\n",
        "best_threshold = thresholds[best_idx]\n",
        "\n",
        "print(\"Best threshold:\", best_threshold)\n",
        "\n",
        "print(\"After tuning threshold.. \")\n",
        "y_pred_tuned = (y_probs >= best_threshold).astype(int)\n",
        "print(classification_report(y_val, y_pred_tuned))\n",
        "ap = average_precision_score(y_val, y_probs)\n",
        "print(\"PR-AUC:\", ap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jDM3XYMeQFK"
      },
      "source": [
        "this classifier has pretty good precision, but we'd like to see if we could improve the recall without destroying precision.\n",
        "\n",
        "\n",
        "Lets try adding class weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjdbOwrBehlC"
      },
      "outputs": [],
      "source": [
        "random_forest_weighted = RandomForestClassifier(class_weight=\"balanced\")\n",
        "random_forest_weighted.fit(x_train, y_train)\n",
        "\n",
        "print(classification_report(y_val, random_forest_weighted.predict(x_val), target_names=['Legit', 'Fraud']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROPIxVwfB6m5"
      },
      "source": [
        "Let's try SMOTE + Random Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDgNmTa6B--k"
      },
      "outputs": [],
      "source": [
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# define pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('rf', RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# train (SMOTE applied ONLY to training data)\n",
        "pipeline.fit(x_train, y_train)\n",
        "\n",
        "# predict\n",
        "y_pred = pipeline.predict(x_val)\n",
        "\n",
        "print(classification_report(y_val, y_pred))\n",
        "print(\"PR-AUC:\", ap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INopRRznIBrB"
      },
      "outputs": [],
      "source": [
        "y_probs = pipeline.predict_proba(x_val)[:, 1]\n",
        "precision, recall, thresholds = precision_recall_curve(y_val, y_probs)\n",
        "\n",
        "#choose threshold that maximizes F1\n",
        "f1 = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "best_idx = np.argmax(f1)\n",
        "best_threshold = thresholds[best_idx]\n",
        "\n",
        "print(\"Best threshold:\", best_threshold)\n",
        "\n",
        "print(\"After tuning threshold.. \")\n",
        "y_pred_tuned = (y_probs >= best_threshold).astype(int)\n",
        "print(classification_report(y_val, y_pred_tuned))\n",
        "\n",
        "ap = average_precision_score(y_val, y_probs)\n",
        "print(\"PR-AUC:\", ap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vQT-pimOPPp"
      },
      "source": [
        "Another experiment with RF + SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR6zc9INOS3S"
      },
      "outputs": [],
      "source": [
        "rf_model = Pipeline([\n",
        "    (\"smote\", SMOTE(\n",
        "        sampling_strategy=0.3,   # not full balance → prevents overfitting\n",
        "        k_neighbors=5,\n",
        "        random_state=42\n",
        "    )),\n",
        "    (\"rf\", RandomForestClassifier(\n",
        "        n_estimators=400,\n",
        "        max_depth=12,\n",
        "        min_samples_leaf=2,\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "#fit model\n",
        "rf_model.fit(x_train, y_train)\n",
        "\n",
        "y_probs = rf_model.predict_proba(x_val)[:, 1]\n",
        "#threshold tuning\n",
        "precision, recall, thresholds = precision_recall_curve(y_val, y_probs)\n",
        "\n",
        "f1 = 2 * precision * recall / (precision + recall + 1e-9)\n",
        "best_idx = np.argmax(f1)\n",
        "best_threshold = thresholds[best_idx]\n",
        "\n",
        "print(\"Best threshold:\", best_threshold)\n",
        "\n",
        "y_pred = (y_probs >= best_threshold).astype(int)\n",
        "\n",
        "print(classification_report(y_val, y_pred))\n",
        "\n",
        "pr_auc = average_precision_score(y_val, y_probs)\n",
        "print(\"PR-AUC:\", pr_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjSebs2kPBC5"
      },
      "source": [
        "Cross-validated training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0Hfy83Wnt-b"
      },
      "outputs": [],
      "source": [
        "#gridsearchCV\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\": [200, 400],\n",
        "    \"max_depth\": [8, 12, None],\n",
        "    \"min_samples_leaf\": [1, 2, 5]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"average_precision\",   # ⭐ THIS LINE\n",
        "    cv=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid.fit(x_train, y_train)\n",
        "\n",
        "print(\"Best PR-AUC:\", grid.best_score_)\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "\n",
        "best_model = grid.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5ZOy037PEcm"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "scores = cross_val_score(\n",
        "    rf_model,\n",
        "    x_train,\n",
        "    y_train,\n",
        "    scoring=\"average_precision\",\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"CV PR-AUC:\", scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQBpHQwzgHcM"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2qYFivJgAw_"
      },
      "source": [
        "#NEURAL NETWORK - CHECK AGAIN!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KszOFII_W2M5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers, callbacks\n",
        "\n",
        "# --- Class weight to handle imbalance ---\n",
        "\n",
        "shallow_nn = keras.models.Sequential([\n",
        "    layers.InputLayer(shape=(x_train.shape[1],)),\n",
        "\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "neg, pos = np.bincount(y_train.astype(int))\n",
        "class_weight = {0: 1.0, 1: (neg / pos) * 0.3}\n",
        "\n",
        "early_stop = callbacks.EarlyStopping(\n",
        "    monitor='val_auc', patience=10,\n",
        "    mode='max', restore_best_weights=True\n",
        ")\n",
        "\n",
        "\n",
        "checkpoint = callbacks.ModelCheckpoint(\n",
        "    'shallow_nn.keras',                              # fix: needs .keras extension\n",
        "    monitor='val_auc',                               # fix: monitor AUC not loss\n",
        "    save_best_only=True,\n",
        "    mode='max'                                       # fix: higher AUC = better\n",
        ")\n",
        "\n",
        "shallow_nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\n",
        "        keras.metrics.AUC(name='auc', curve='PR')])\n",
        "shallow_nn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2T2hKNjZnKH"
      },
      "outputs": [],
      "source": [
        "history = shallow_nn.fit(\n",
        "    x_train, y_train,\n",
        "    validation_data=(x_val, y_val),\n",
        "    epochs=100,          # let early stopping decide when to quit\n",
        "    batch_size=2048,     # larger batches = faster epochs, more stable gradients\n",
        "    class_weight=class_weight,\n",
        "    callbacks=[checkpoint, early_stop]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjdge0yO7zxu"
      },
      "source": [
        "# todo: try using algorithmic threshold split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOrHv1wqdhG7"
      },
      "outputs": [],
      "source": [
        "y_proba = shallow_nn.predict(x_val)\n",
        "y_pred = (y_proba > 0.8).astype(int)\n",
        "print(classification_report(y_val, y_pred, target_names=['Legit', 'Fraud']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-74KU2MJxq3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Get validation probabilities (risk scores)\n",
        "y_proba = shallow_nn.predict(x_val).ravel()\n",
        "\n",
        "# Compute precision-recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_val, y_proba)\n",
        "\n",
        "# Compute F1 scores\n",
        "f1 = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "\n",
        "# Find best threshold\n",
        "best_idx = np.argmax(f1)\n",
        "best_threshold = thresholds[best_idx]\n",
        "\n",
        "print(\"Best threshold (validation):\", best_threshold)\n",
        "\n",
        "# Apply optimal threshold\n",
        "y_pred_optimal = (y_proba > best_threshold).astype(int)\n",
        "\n",
        "print(\"\\nValidation performance with optimal threshold:\")\n",
        "print(classification_report(y_val, y_pred_optimal, target_names=['Legit', 'Fraud']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy-zFVIwe_mJ"
      },
      "source": [
        "After increasing network capacity and adjusting class weights, the neural network achieved high recall (0.92) while maintaining strong precision (0.77).\n",
        "This demonstrates that proper imbalance handling and architecture tuning significantly improve performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYaXuzaQTftY"
      },
      "source": [
        "# XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "051nvx-TT9Am"
      },
      "outputs": [],
      "source": [
        "pip install xgboost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwGkrI29Um29"
      },
      "outputs": [],
      "source": [
        "neg = (y_train == 0).sum()\n",
        "pos = (y_train == 1).sum()\n",
        "\n",
        "scale_pos_weight = neg / pos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrimze_FXGdW"
      },
      "source": [
        "We computed the number of legitimate and fraud transactions in the training set.\n",
        "Since the dataset is highly imbalanced, we calculated scale_pos_weight to penalize mistakes on the minority (fraud) class more heavily.\n",
        "This helps XGBoost focus more on detecting fraud cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzbu1AMXT3Su"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Calculate pos_weight using the already defined neg and pos counts\n",
        "pos_weight = neg / pos\n",
        "\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=4,\n",
        "    scale_pos_weight=pos_weight,  # important for imbalance\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Correcting variable names from X_train, X_test to x_train, x_test\n",
        "xgb.fit(x_train, y_train)\n",
        "\n",
        "y_pred = xgb.predict(x_val)\n",
        "y_proba = xgb.predict_proba(x_val)[:, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJC3Id7YUNEd"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, average_precision_score\n",
        "\n",
        "print(confusion_matrix(y_val, y_pred))\n",
        "print(classification_report(y_val, y_pred))\n",
        "\n",
        "pr_auc = average_precision_score(y_val, y_pred)\n",
        "print(\"PR-AUC:\", pr_auc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StbQpzZLKHPL"
      },
      "outputs": [],
      "source": [
        "#Threshold tuning the XGBoost Model\n",
        "precision, recall, thresholds = precision_recall_curve(y_val, y_proba)\n",
        "\n",
        "#choose threshold that maximizes F1\n",
        "f1 = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "best_idx = np.argmax(f1)\n",
        "best_threshold = thresholds[best_idx]\n",
        "\n",
        "print(\"Best threshold:\", best_threshold)\n",
        "\n",
        "print(\"After tuning threshold.. \")\n",
        "y_pred_tuned = (y_proba >= best_threshold).astype(int)\n",
        "print(classification_report(y_val, y_pred_tuned))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}